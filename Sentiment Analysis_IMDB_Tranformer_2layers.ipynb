{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aab9a569",
      "metadata": {
        "id": "aab9a569"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8190b19",
      "metadata": {
        "id": "b8190b19"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DvQNvnr02MC",
        "outputId": "5461b552-3e2c-4e27-fe75-3b8825b1526d"
      },
      "id": "5DvQNvnr02MC",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 11s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdbEr.txt', 'train', 'imdb.vocab', 'README', 'test']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA4T9l6R02OX",
        "outputId": "6a5e85c3-5b92-4435-b07f-965ae0556cf1"
      },
      "id": "BA4T9l6R02OX",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['urls_unsup.txt',\n",
              " 'labeledBow.feat',\n",
              " 'neg',\n",
              " 'urls_neg.txt',\n",
              " 'unsup',\n",
              " 'unsupBow.feat',\n",
              " 'urls_pos.txt',\n",
              " 'pos']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "metadata": {
        "id": "Pjth0RHA02S4"
      },
      "id": "Pjth0RHA02S4",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfNwv9QT02VX"
      },
      "id": "VfNwv9QT02VX",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FONX7HQ402X5"
      },
      "id": "FONX7HQ402X5",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fafaf36e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fafaf36e",
        "outputId": "1292f272-df25-48e6-ebc6-8c90fb6f3aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 1024\n",
        "seed = 12345\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "                            'aclImdb/train', batch_size=batch_size, \n",
        "                            validation_split=0.2,\n",
        "                            subset='training', seed=seed)\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "                            'aclImdb/train', batch_size=batch_size, \n",
        "                            validation_split=0.2,\n",
        "                            subset='validation', seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fed87acf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fed87acf",
        "outputId": "5ac81f72-9bf6-4ebf-f0f1-8b8bc2e627dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "vocab_size   = 20000\n",
        "sequence_len = 200\n",
        "\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "vectorization = tf.keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_len,\n",
        ")\n",
        "\n",
        "vectorization.adapt(train_ds.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eaa2d5a2",
      "metadata": {
        "id": "eaa2d5a2"
      },
      "outputs": [],
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorization(text), label\n",
        "\n",
        "train_ds = train_ds.map(vectorize_text)\n",
        "val_ds = val_ds.map(vectorize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2486508b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2486508b",
        "outputId": "9fc96d62-0ca1-4db0-e162-e9683b4c8552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "[ 2340  4688   447     1     2     1  7756  4688   674     1     2     1\n",
            "   167     1     5  5953     3  2730  2050  3250    14 16623    14  7824\n",
            " 17408   510  2886     5 16708     1   967 15850 10635    31     2  1413\n",
            "     5    29     1 15602  2177   293    33 12974  3768     5   656    35\n",
            "     2  1947  3275  1753  1719     6   938    11  7175   111   619     6\n",
            "    65 14406     2  3958  9106     5     2 10635    24  1264   203  1305\n",
            "     8  4970  7573  3367     2   676     5   253   230     6  7948   154\n",
            " 17408   379  1440   350     5  1041 16824     3  5459  1619   450     2\n",
            "   278  2482    16  4863  3678   982  2289     3  6520  9790   742  3863\n",
            "   881   300     6     2  7856 15594     5   619   846     3 11474   230\n",
            "  5838     5     1 18551   450     2  1988  9394     2  1674     5     2\n",
            "   223     6    29    85  5080 15602 17921    30     1    20    23   904\n",
            "    35     2   295  9632     5  3604  9614  3795     8  1831  2411     6\n",
            "   467     1  1152    16    65 18858     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in train_ds:\n",
        "    print(label_batch[0].numpy())\n",
        "    print(text_batch.numpy()[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2a8a2783",
      "metadata": {
        "id": "2a8a2783"
      },
      "outputs": [],
      "source": [
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc351cc",
      "metadata": {
        "id": "acc351cc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5484718e",
      "metadata": {
        "id": "5484718e"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cffe32c7",
      "metadata": {
        "id": "cffe32c7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1a727afd",
      "metadata": {
        "id": "1a727afd"
      },
      "outputs": [],
      "source": [
        "# Two seperate embedding layers, one for tokens, one for token index (positions)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "68667876",
      "metadata": {
        "id": "68667876"
      },
      "outputs": [],
      "source": [
        "embed_dim = 128  # Embedding size for each token\n",
        "num_heads = 6    # Number of attention heads\n",
        "ff_dim = 128     # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "embedding_layer = TokenAndPositionEmbedding(sequence_len, vocab_size, embed_dim)\n",
        "transformer_block1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "transformer_block2 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "\n",
        "inputs = layers.Input(shape=(sequence_len,))\n",
        "x = embedding_layer(inputs)\n",
        "x = transformer_block1(x)\n",
        "x = transformer_block2(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0cd01ada",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cd01ada",
        "outputId": "23ae8fd4-fca1-45e8-f071-4ce7f7aa8139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "20/20 [==============================] - 36s 1s/step - loss: 0.7353 - accuracy: 0.5130 - val_loss: 0.6882 - val_accuracy: 0.4920\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.6315 - accuracy: 0.6304 - val_loss: 0.4477 - val_accuracy: 0.7974\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 24s 1s/step - loss: 0.3397 - accuracy: 0.8569 - val_loss: 0.3100 - val_accuracy: 0.8790\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_ds, batch_size=32, epochs=3, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9a0f48c",
      "metadata": {
        "id": "e9a0f48c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4df51c9",
      "metadata": {
        "id": "f4df51c9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}